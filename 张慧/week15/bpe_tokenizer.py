# -*- encoding: utf-8 -*-
"""
@File    : bpe_tokenizer.py
@Time    : 2025/9/10 17:22
@Version : python 3.10
@Author  : Hui Zhang
@Contact : hui.zhang@quanmag.com
@Software: PyCharm
@Description:
"""


class BPETokenizer:
    def __init__(self, vocab=None, merges=None):
        """åˆå§‹åŒ–BPEåˆ†è¯å™¨

        Args:
            vocab (dict): è¯æ±‡è¡¨ï¼Œå°†token IDæ˜ å°„åˆ°å­—èŠ‚åºåˆ—
            merges (dict): åˆå¹¶è§„åˆ™ï¼Œå°†tokenå¯¹æ˜ å°„åˆ°æ–°çš„token ID
        """
        self.vocab = vocab or {idx: bytes([idx]) for idx in range(256)}
        self.merges = merges or {}

    def train(self, text, vocab_size):
        """è®­ç»ƒBPEæ¨¡å‹

        Args:
            text (str): ç”¨äºè®­ç»ƒçš„æ–‡æœ¬
            vocab_size (int): æœŸæœ›çš„è¯æ±‡è¡¨å¤§å°
        """
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºUTF-8å­—èŠ‚ï¼Œç„¶åè½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
        tokens = list(text.encode("utf-8"))

        # è®¡ç®—éœ€è¦åˆå¹¶çš„æ¬¡æ•°
        num_merges = vocab_size - 256
        ids = list(tokens)  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸå§‹åˆ—è¡¨

        # é‡ç½®mergeså’Œvocab
        self.merges = {}
        self.vocab = {idx: bytes([idx]) for idx in range(256)}

        # æ‰§è¡ŒBPEè®­ç»ƒ
        for i in range(num_merges):
            stats = self._get_stats(ids)
            if not stats:
                break
            pair = max(stats, key=stats.get)
            idx = 256 + i
            ids = self._merge(ids, pair, idx)
            self.merges[pair] = idx
            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]

    def encode(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåˆ—è¡¨

        Args:
            text (str): å¾…ç¼–ç çš„æ–‡æœ¬

        Returns:
            list: token IDåˆ—è¡¨
        """
        if not isinstance(text, str):
            raise TypeError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        # å¤„ç†ç©ºå­—ç¬¦ä¸²æƒ…å†µ
        if len(text) == 0:
            return []

        # å°†æ–‡æœ¬è½¬æ¢ä¸ºUTF-8å­—èŠ‚ï¼Œç„¶åè½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
        tokens = list(text.encode("utf-8"))

        # é‡å¤åˆå¹¶æœ€å¸¸è§çš„ç›¸é‚»tokenå¯¹
        while len(tokens) >= 2:
            stats = self._get_stats(tokens)
            if not stats:
                break

            # æ‰¾åˆ°æœ€å¸¸è§çš„tokenå¯¹
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))

            # å¦‚æœè¿™ä¸ªtokenå¯¹ä¸åœ¨mergesä¸­ï¼Œè¯´æ˜æ— æ³•ç»§ç»­åˆå¹¶
            if pair not in self.merges:
                break

            # åˆå¹¶tokenå¯¹
            idx = self.merges[pair]
            tokens = self._merge(tokens, pair, idx)

        return tokens

    def decode(self, ids):
        """å°†token IDåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬

        Args:
            ids (list): token IDåˆ—è¡¨

        Returns:
            str: è§£ç åçš„æ–‡æœ¬
        """
        # å°†token IDè½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
        tokens = b"".join(self.vocab[idx] for idx in ids)

        # å°†å­—èŠ‚åºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²
        text = tokens.decode("utf-8", errors="replace")
        return text

    def _get_stats(self, ids):
        """ç»Ÿè®¡ç›¸é‚»tokenå¯¹çš„é¢‘ç‡

        Args:
            ids (list): token IDåˆ—è¡¨

        Returns:
            dict: tokenå¯¹åˆ°é¢‘ç‡çš„æ˜ å°„
        """
        counts = {}
        for pair in zip(ids, ids[1:]):  # Pythonic way to iterate consecutive elements
            counts[pair] = counts.get(pair, 0) + 1
        return counts

    def _merge(self, ids, pair, idx):
        """åˆå¹¶æŒ‡å®šçš„tokenå¯¹

        Args:
            ids (list): token IDåˆ—è¡¨
            pair (tuple): è¦åˆå¹¶çš„tokenå¯¹
            idx (int): åˆå¹¶åçš„æ–°token ID

        Returns:
            list: åˆå¹¶åçš„token IDåˆ—è¡¨
        """
        newids = []
        i = 0
        while i < len(ids):
            # å¦‚æœæ‰¾åˆ°è¦åˆå¹¶çš„tokenå¯¹
            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:
                newids.append(idx)
                i += 2
            else:
                newids.append(ids[i])
                i += 1
        return newids

    def save(self, file_prefix):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶

        Args:
            file_prefix (str): æ–‡ä»¶å‰ç¼€
        """
        import json

        # ä¿å­˜è¯æ±‡è¡¨
        vocab_file = file_prefix + '.vocab'
        with open(vocab_file, 'w', encoding='utf-8') as f:
            # å°†bytesè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_vocab = {k: v.decode('utf-8', errors='replace') if isinstance(v, bytes) else v
                                  for k, v in self.vocab.items()}
            json.dump(serializable_vocab, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆå¹¶è§„åˆ™
        merges_file = file_prefix + '.merges'
        with open(merges_file, 'w', encoding='utf-8') as f:
            # å°†tupleé”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            serializable_merges = {f"{k[0]},{k[1]}": v for k, v in self.merges.items()}
            json.dump(serializable_merges, f, ensure_ascii=False, indent=2)

    def load(self, file_prefix):
        """ä»æ–‡ä»¶åŠ è½½æ¨¡å‹

        Args:
            file_prefix (str): æ–‡ä»¶å‰ç¼€
        """
        import json

        # åŠ è½½è¯æ±‡è¡¨
        vocab_file = file_prefix + '.vocab'
        with open(vocab_file, 'r', encoding='utf-8') as f:
            loaded_vocab = json.load(f)
            # å°†å­—ç¬¦ä¸²è½¬æ¢å›bytes
            self.vocab = {int(k): v.encode('utf-8') if isinstance(v, str) else v
                          for k, v in loaded_vocab.items()}

        # åŠ è½½åˆå¹¶è§„åˆ™
        merges_file = file_prefix + '.merges'
        with open(merges_file, 'r', encoding='utf-8') as f:
            loaded_merges = json.load(f)
            # å°†å­—ç¬¦ä¸²é”®è½¬æ¢å›tuple
            self.merges = {tuple(map(int, k.split(','))): v for k, v in loaded_merges.items()}


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
    tokenizer = BPETokenizer()

    # è®­ç»ƒæ–‡æœ¬
    text = "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception."

    # è®­ç»ƒæ¨¡å‹
    tokenizer.train(text, 300)

    # æµ‹è¯•ç¼–ç å’Œè§£ç 
    test_text = "A Programmerâ€™s Introduction to Unicode"
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"åŸå§‹æ–‡æœ¬: {test_text}")
    print(f"ç¼–ç ç»“æœ: {encoded}")
    print(f"è§£ç ç»“æœ: {decoded}")
    print(f"ç¼–ç è§£ç æ˜¯å¦ä¸€è‡´: {test_text == decoded}")

    # ä¿å­˜æ¨¡å‹
    tokenizer.save("bpe_model")

    # åŠ è½½æ¨¡å‹
    new_tokenizer = BPETokenizer()
    new_tokenizer.load("bpe_model")

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    encoded2 = new_tokenizer.encode(test_text)
    print(f"åŠ è½½æ¨¡å‹åçš„ç¼–ç ç»“æœ: {encoded2}")
    print(f"ä¸¤æ¬¡ç¼–ç ç»“æœæ˜¯å¦ä¸€è‡´: {encoded == encoded2}")
